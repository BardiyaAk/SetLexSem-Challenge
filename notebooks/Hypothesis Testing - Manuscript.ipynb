{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "\n",
    "from setlexsem.analyze.hypothesis_testing_utils import (\n",
    "    add_nl,\n",
    "    add_text,\n",
    "    agg,\n",
    "    concat_sets,\n",
    "    create_fig_path,\n",
    "    create_filtered_df_for_hypothesis,\n",
    "    get_config,\n",
    "    get_stats,\n",
    "    save_config_and_data,\n",
    ")\n",
    "from setlexsem.analyze.visualize import create_violin_agg, viz_barplot\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "from setlexsem.constants import (\n",
    "    PATH_HYPOTHESIS_CONFIG_ROOT,\n",
    "    PATH_POSTPROCESS,\n",
    "    PATH_ROOT,\n",
    "    STUDY2DECEPTIVE_WORD_SAMPLER,\n",
    "    STUDY2MODEL,\n",
    "    TOKEN_ORDER,\n",
    ")\n",
    "from setlexsem.utils import convert_model_name, load_processed_data, make_nice\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# location to store the results as table and\n",
    "FOLDER_NAME = \"camera_ready_figures\"\n",
    "SUPPLEMENTARY_ROOT = os.path.join(\n",
    "    PATH_ROOT, \"manuscript/supplementary_materials_camera_ready/experiments\"\n",
    ")\n",
    "os.makedirs(SUPPLEMENTARY_ROOT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDY_LIST = list(STUDY2MODEL.keys())\n",
    "\n",
    "df_results = pd.DataFrame()\n",
    "for study_name in STUDY_LIST:\n",
    "    csv_path = os.path.join(PATH_POSTPROCESS, f\"{study_name}.csv\")\n",
    "    df_temp = load_processed_data(csv_path)\n",
    "    df_results = pd.concat([df_results, df_temp]).reset_index(drop=True)\n",
    "\n",
    "df_results, ugly_map = make_nice(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    os.path.join(PATH_HYPOTHESIS_CONFIG_ROOT, \"hypothesis.json\"), \"r\"\n",
    ") as f:\n",
    "    hypothesis_configs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis: Prompt Language Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = \"Prompt Language Comparison\"\n",
    "hypothesis_config = hypothesis_configs[hypothesis]\n",
    "hypo_name = hypothesis_config[\"hypo_name\"]\n",
    "\n",
    "df_new = create_filtered_df_for_hypothesis(df_results, hypothesis_config)\n",
    "\n",
    "fig = create_violin_agg(\n",
    "    df_new,\n",
    "    x_name=\"Demonstration phrasing\",\n",
    "    figure_size=(14, 4),\n",
    "    save_fig=create_fig_path(hypo_name, folder=FOLDER_NAME),\n",
    "    fontsize=18,\n",
    "    split_bar=False,\n",
    "    save_raw_experiment=1,\n",
    "    supp_root=SUPPLEMENTARY_ROOT,\n",
    ")\n",
    "\n",
    "save_config_and_data(df_new, hypo_name, SUPPLEMENTARY_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis: Operation Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = \"Operation Type Comparison\"\n",
    "hypothesis_config = hypothesis_configs[hypothesis]\n",
    "hypo_name = hypothesis_config[\"hypo_name\"]\n",
    "\n",
    "df_new = create_filtered_df_for_hypothesis(df_results, hypothesis_config)\n",
    "\n",
    "fig = create_violin_agg(\n",
    "    df_new,\n",
    "    x_name=\"Set operation\",\n",
    "    figure_size=(7, 4),\n",
    "    save_fig=create_fig_path(hypo_name, folder=FOLDER_NAME),\n",
    "    split_bar=False,\n",
    "    fontsize=21,\n",
    "    save_raw_experiment=1,\n",
    "    supp_root=SUPPLEMENTARY_ROOT,\n",
    ")\n",
    "\n",
    "save_config_and_data(df_new, hypo_name, SUPPLEMENTARY_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis: Numbers vs Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypo_name = \"Numbers vs Words\"\n",
    "hypothesis_config = hypothesis_configs[hypothesis]\n",
    "hypo_name = hypothesis_config[\"hypo_name\"]\n",
    "\n",
    "df_new = create_filtered_df_for_hypothesis(df_results, hypothesis_config)\n",
    "\n",
    "fig = create_violin_agg(\n",
    "    df_new,\n",
    "    x_name=\"Set operation\",\n",
    "    hue=\"Token type\",\n",
    "    figure_size=(8, 4),\n",
    "    save_fig=create_fig_path(hypo_name, folder=FOLDER_NAME),\n",
    "    legend_loc=\"lower right\",\n",
    "    fontsize=18,\n",
    "    save_raw_experiment=1,\n",
    "    supp_root=SUPPLEMENTARY_ROOT,\n",
    ")\n",
    "\n",
    "save_config_and_data(df_new, hypo_name, SUPPLEMENTARY_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis: Operand Size (filtered to include all LLMs besides Haiku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = \"Operand Size\"\n",
    "hypothesis_config = hypothesis_configs[hypothesis]\n",
    "hypo_name = hypothesis_config[\"hypo_name\"]\n",
    "\n",
    "df_new = create_filtered_df_for_hypothesis(df_results, hypothesis_config)\n",
    "\n",
    "# Add new line\n",
    "df_new[\"Operand size\"] = df_new[\"Operand size\"].apply(add_nl)\n",
    "\n",
    "hypo_name = \"operand_size\"\n",
    "fig = create_violin_agg(\n",
    "    df_new,\n",
    "    x_name=\"Operand size\",\n",
    "    figure_size=(8, 4),\n",
    "    save_fig=create_fig_path(hypo_name, folder=FOLDER_NAME),\n",
    "    split_bar=False,\n",
    "    fontsize=18,\n",
    "    save_raw_experiment=1,\n",
    "    supp_root=SUPPLEMENTARY_ROOT,\n",
    ")\n",
    "\n",
    "save_config_and_data(df_new, hypo_name, SUPPLEMENTARY_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis: LLM Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = \"LLM Comparison\"\n",
    "hypothesis_config = hypothesis_configs[hypothesis]\n",
    "hypo_name = hypothesis_config[\"hypo_name\"]\n",
    "\n",
    "df_new = create_filtered_df_for_hypothesis(df_results, hypothesis_config)\n",
    "\n",
    "fig = create_violin_agg(\n",
    "    df_new,\n",
    "    x_name=\"LLM\",\n",
    "    figure_size=(12, 4),\n",
    "    save_fig=create_fig_path(hypo_name, folder=FOLDER_NAME),\n",
    "    split_bar=False,\n",
    "    fontsize=14,\n",
    "    save_raw_experiment=1,\n",
    "    supp_root=SUPPLEMENTARY_ROOT,\n",
    ")\n",
    "\n",
    "save_config_and_data(df_new, hypo_name, SUPPLEMENTARY_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis: Token Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = \"Token Frequency\"\n",
    "hypothesis_config = hypothesis_configs[hypothesis]\n",
    "hypo_name = hypothesis_config[\"hypo_name\"]\n",
    "\n",
    "df_new = create_filtered_df_for_hypothesis(df_results, hypothesis_config)\n",
    "\n",
    "\n",
    "df_new[\"Token frequency\"] = df_new[\"Token frequency\"].apply(add_text)\n",
    "df_new[\"Token frequency\"] = df_new[\"Token frequency\"].astype(\"category\")\n",
    "df_new[\"Token frequency\"] = df_new[\"Token frequency\"].cat.set_categories(\n",
    "    TOKEN_ORDER, ordered=True\n",
    ")\n",
    "\n",
    "fig = create_violin_agg(\n",
    "    df_new,\n",
    "    x_name=\"Set operation\",\n",
    "    hue=\"Token frequency\",\n",
    "    figure_size=(12, 4),\n",
    "    save_fig=create_fig_path(hypo_name, folder=FOLDER_NAME),\n",
    "    split_bar=False,\n",
    "    legend_loc=\"outer right\",\n",
    "    save_raw_experiment=1,\n",
    "    supp_root=SUPPLEMENTARY_ROOT,\n",
    ")\n",
    "\n",
    "save_config_and_data(df_new, hypo_name, SUPPLEMENTARY_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciles: Distribution Plots & Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a figure and a grid of subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(8, 4), dpi=600)\n",
    "axes = axes.flatten()  # Flatten the axes to make it a 1D array\n",
    "\n",
    "full_accuracy = []\n",
    "for i, token_freq in enumerate(TOKEN_ORDER[1:]):\n",
    "\n",
    "    a = df_new[\n",
    "        (df_new[\"Token length\"] == 3)\n",
    "        & (df_new[\"Token frequency\"] == token_freq)\n",
    "    ]\n",
    "    b = df_new[\n",
    "        (df_new[\"Token length\"] == 5)\n",
    "        & (df_new[\"Token frequency\"] == token_freq)\n",
    "    ]\n",
    "\n",
    "    # merge by columns and then add \"Accuracy\" column back\n",
    "    c = a.merge(\n",
    "        b,\n",
    "        on=[\n",
    "            \"Token type\",\n",
    "            \"Set operation\",\n",
    "            \"Demonstration phrasing\",\n",
    "            \"Prompting method\",\n",
    "            \"Operand size\",\n",
    "            \"Number of demonstrations\",\n",
    "            \"Max Value\",\n",
    "            \"Token similarity\",\n",
    "            \"Token frequency\",\n",
    "            \"Relationship between sets A and B\",\n",
    "            \"N Samples\",\n",
    "            \"LLM\",\n",
    "        ],\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"_tl_3\", \"_tl_5\"),\n",
    "    )\n",
    "    c[\"Accuracy\"] = c[\"Avg Accuracy_tl_5\"] - c[\"Avg Accuracy_tl_3\"]\n",
    "\n",
    "    full_accuracy.append(\n",
    "        {\n",
    "            \"decile\": token_freq,\n",
    "            \"accuracy_diff_mean\": c[\"Accuracy\"].mean(),\n",
    "            \"accuracy_diff_std\": c[\"Accuracy\"].std(),\n",
    "            \"accuracy_diff_min\": c[\"Accuracy\"].min(),\n",
    "            \"accuracy_diff_max\": c[\"Accuracy\"].max(),\n",
    "            \"n_comparisons\": len(c),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Plot the histogram\n",
    "    if i < len(axes):\n",
    "        # get binwidth fixed\n",
    "        sns.histplot(data=c, x=\"Accuracy\", ax=axes[i], kde=True, bins=20)\n",
    "        axes[i].set_title(f\"{token_freq}\")\n",
    "        axes[i].set_xlim(-82, 82)\n",
    "        axes[i].set_ylim(0, 50)\n",
    "        if i != 0 and i != 4:\n",
    "            # remove ytick and ylabel\n",
    "            axes[i].set_yticks([])\n",
    "            axes[i].set_ylabel(\"\")\n",
    "        if i < 4:\n",
    "            axes[i].set_xticks([])\n",
    "            axes[i].set_xlabel(\"\")\n",
    "        else:\n",
    "            axes[i].set_xticks([-60, -30, 0, 30, 60])\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "fontsize = 18\n",
    "# set fontsize\n",
    "for ax in fig.axes:\n",
    "    for item in (\n",
    "        [ax.title, ax.xaxis.label, ax.yaxis.label]\n",
    "        # + ax.get_xticklabels()\n",
    "        # + ax.get_yticklabels()\n",
    "    ):\n",
    "        item.set_fontsize(fontsize)\n",
    "\n",
    "fontsize = 14\n",
    "# set fontsize\n",
    "for ax in fig.axes:\n",
    "    for item in [] + ax.get_xticklabels() + ax.get_yticklabels():\n",
    "        item.set_fontsize(fontsize)\n",
    "\n",
    "# plt.subplots_adjust(hspace=0.2)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "fig.savefig(\n",
    "    create_fig_path(\"deciles_token_length\", folder=FOLDER_NAME),\n",
    "    bbox_inches=\"tight\",\n",
    "    backend=\"pdf\",\n",
    ")\n",
    "\n",
    "df_deciles = pd.DataFrame(full_accuracy).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis: Deceptiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = \"Deceptiveness Swapping Effect\"\n",
    "hypothesis_config = hypothesis_configs[hypothesis]\n",
    "hypo_name = hypothesis_config[\"hypo_name\"]\n",
    "\n",
    "df_new = create_filtered_df_for_hypothesis(df_results, hypothesis_config)\n",
    "\n",
    "df_new[\"Relationship between sets A and B\"] = (\n",
    "    df_new[\"Relationship between sets A and B\"]\n",
    "    .replace(\n",
    "        {\n",
    "            0: \"Semantically disjoint\",\n",
    "            1: \"Semantically intermingled\",\n",
    "        }\n",
    "    )\n",
    "    .astype(\"category\")\n",
    ")\n",
    "# order these categories\n",
    "df_new[\"Relationship between sets A and B\"] = df_new[\n",
    "    \"Relationship between sets A and B\"\n",
    "].cat.reorder_categories(\n",
    "    [\"Semantically disjoint\", \"Semantically intermingled\"]\n",
    ")\n",
    "\n",
    "\n",
    "fig = create_violin_agg(\n",
    "    df_new,\n",
    "    x_name=\"Set operation\",  # \"Operand size\",\n",
    "    hue=\"Relationship between sets A and B\",\n",
    "    figure_size=(12, 6),\n",
    "    save_fig=create_fig_path(hypo_name, folder=FOLDER_NAME),\n",
    "    split_bar=True,\n",
    "    save_raw_experiment=1,\n",
    "    supp_root=SUPPLEMENTARY_ROOT,\n",
    ")\n",
    "\n",
    "save_config_and_data(df_new, hypo_name, SUPPLEMENTARY_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[\"Token similarity\"] = df_new[\"Token similarity\"].replace(\n",
    "    {\n",
    "        0: \"No semantic manipulation\",\n",
    "        1: \"Intermingled sets with similar semantic words\",\n",
    "    }\n",
    ")\n",
    "df_new = df_new.drop(columns=[\"Relationship between sets A and B\"])\n",
    "df_new = df_new.rename(\n",
    "    columns={\"Token similarity\": \"Relationship between sets A and B\"}\n",
    ")\n",
    "\n",
    "fig = create_violin_agg(\n",
    "    df_new,\n",
    "    x_name=\"Set operation\",  # \"Operand size\",\n",
    "    hue=\"Relationship between sets A and B\",\n",
    "    figure_size=(12, 6),\n",
    "    save_fig=create_fig_path(hypo_name, folder=FOLDER_NAME),\n",
    "    split_bar=True,\n",
    "    save_raw_experiment=1,\n",
    "    supp_root=SUPPLEMENTARY_ROOT,\n",
    ")\n",
    "\n",
    "save_config_and_data(df_new, hypo_name, SUPPLEMENTARY_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = \"Deceptiveness Effect K Shots\"\n",
    "hypothesis_config = hypothesis_configs[hypothesis]\n",
    "hypo_name = hypothesis_config[\"hypo_name\"]\n",
    "\n",
    "\n",
    "df_new = create_filtered_df_for_hypothesis(df_results, hypothesis_config)\n",
    "\n",
    "df_new[\"Relationship between sets A and B\"] = (\n",
    "    df_new[\"Relationship between sets A and B\"]\n",
    "    .replace(\n",
    "        {\n",
    "            0: \"Semantically disjoint\",\n",
    "            1: \"Semantically intermingled\",\n",
    "        }\n",
    "    )\n",
    "    .astype(\"category\")\n",
    ")\n",
    "fig = create_violin_agg(\n",
    "    df_new,\n",
    "    x_name=\"Number of demonstrations\",  # \"Operand size\",\n",
    "    hue=\"Relationship between sets A and B\",\n",
    "    figure_size=(12, 6),\n",
    "    save_fig=create_fig_path(hypo_name, folder=FOLDER_NAME),\n",
    "    split_bar=True,\n",
    "    save_raw_experiment=1,\n",
    "    supp_root=SUPPLEMENTARY_ROOT,\n",
    ")\n",
    "\n",
    "save_config_and_data(df_new, hypo_name, SUPPLEMENTARY_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Source(Deceptive Words for Word Sampler (Random Baseline vs Overlap Sampler vs Non-Overlap Sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = \"Deceptiveness Effect For Word and Overlap Samplers\"\n",
    "hypothesis_config = hypothesis_configs[hypothesis]\n",
    "hypo_name = hypothesis_config[\"hypo_name\"]\n",
    "\n",
    "df_new = create_filtered_df_for_hypothesis(df_results, hypothesis_config)\n",
    "\n",
    "df_new[\"Word sampler sampling method\"] = df_new[\"Study Name\"].apply(\n",
    "    lambda x: STUDY2DECEPTIVE_WORD_SAMPLER[x]\n",
    ")\n",
    "\n",
    "fig = create_violin_agg(\n",
    "    df_new,\n",
    "    x_name=\"Word sampler sampling method\",\n",
    "    figure_size=(12, 6),\n",
    "    save_fig=create_fig_path(hypo_name),\n",
    "    split_bar=False,\n",
    "    save_raw_experiment=1,\n",
    "    supp_root=SUPPLEMENTARY_ROOT,\n",
    ")\n",
    "\n",
    "save_config_and_data(df_new, hypo_name, SUPPLEMENTARY_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = \"Deceptiveness Effect For Word and Overlap Samplers For Each Set Operation\"\n",
    "hypothesis_config = hypothesis_configs[hypothesis]\n",
    "hypo_name = hypothesis_config[\"hypo_name\"]\n",
    "\n",
    "df_new = create_filtered_df_for_hypothesis(df_results, hypothesis_config)\n",
    "\n",
    "df_new[\"Word sampler sampling method\"] = df_new[\"Study Name\"].apply(\n",
    "    lambda x: STUDY2DECEPTIVE_WORD_SAMPLER[x]\n",
    ")\n",
    "fig = create_violin_agg(\n",
    "    df_new,\n",
    "    x_name=\"Set operation\",\n",
    "    hue=\"Word sampler sampling method\",\n",
    "    figure_size=(14, 8),\n",
    "    save_fig=create_fig_path(hypo_name),\n",
    "    split_bar=False,\n",
    "    save_raw_experiment=1,\n",
    "    supp_root=SUPPLEMENTARY_ROOT,\n",
    ")\n",
    "\n",
    "save_config_and_data(df_new, hypo_name, SUPPLEMENTARY_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis: Overlap Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numbers\n",
    "\n",
    "hypothesis = \"Is Overlap Important in Numbers?\"\n",
    "hypothesis_config = hypothesis_configs[hypothesis]\n",
    "hypo_name = hypothesis_config[\"hypo_name\"]\n",
    "\n",
    "df_new = create_filtered_df_for_hypothesis(df_results, hypothesis_config)\n",
    "\n",
    "fig = create_violin_agg(\n",
    "    df_new,\n",
    "    x_name=\"Set operation\",\n",
    "    hue=\"Token type\",\n",
    "    figure_size=(8, 4),\n",
    "    save_fig=create_fig_path(hypo_name, folder=FOLDER_NAME),\n",
    "    split_bar=True,\n",
    "    fontsize=18,\n",
    "    save_raw_experiment=1,\n",
    "    supp_root=SUPPLEMENTARY_ROOT,\n",
    ")\n",
    "\n",
    "save_config_and_data(df_new, hypo_name, SUPPLEMENTARY_ROOT)\n",
    "\n",
    "# hypo_overall\n",
    "fig = create_violin_agg(\n",
    "    df_new,\n",
    "    x_name=\"Token type\",\n",
    "    figure_size=(6, 4),\n",
    "    save_fig=create_fig_path(hypo_name + \"_overall\", folder=FOLDER_NAME),\n",
    "    split_bar=False,\n",
    "    fontsize=18,\n",
    "    save_raw_experiment=1,\n",
    "    supp_root=SUPPLEMENTARY_ROOT,\n",
    ")\n",
    "\n",
    "save_config_and_data(df_new, hypo_name + \"_overall\", SUPPLEMENTARY_ROOT)\n",
    "\n",
    "\n",
    "## Words\n",
    "hypothesis = \"Is Overlap Important in Words?\"\n",
    "hypothesis_config = hypothesis_configs[hypothesis]\n",
    "hypo_name = hypothesis_config[\"hypo_name\"]\n",
    "\n",
    "df_new = create_filtered_df_for_hypothesis(df_results, hypothesis_config)\n",
    "\n",
    "fig = create_violin_agg(\n",
    "    df_new,\n",
    "    x_name=\"Set operation\",\n",
    "    hue=\"Token type\",\n",
    "    figure_size=(8, 4),\n",
    "    save_fig=create_fig_path(hypo_name, folder=FOLDER_NAME),\n",
    "    split_bar=True,\n",
    "    fontsize=18,\n",
    "    save_raw_experiment=1,\n",
    "    supp_root=SUPPLEMENTARY_ROOT,\n",
    ")\n",
    "\n",
    "save_config_and_data(df_new, hypo_name, SUPPLEMENTARY_ROOT)\n",
    "\n",
    "# hypo_overall\n",
    "fig = create_violin_agg(\n",
    "    df_new,\n",
    "    x_name=\"Token type\",\n",
    "    figure_size=(6, 4),\n",
    "    save_fig=create_fig_path(hypo_name + \"_overall\", folder=FOLDER_NAME),\n",
    "    split_bar=False,\n",
    "    fontsize=18,\n",
    "    save_raw_experiment=1,\n",
    "    supp_root=SUPPLEMENTARY_ROOT,\n",
    ")\n",
    "\n",
    "save_config_and_data(df_new, hypo_name + \"_overall\", SUPPLEMENTARY_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciles: Count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "\n",
    "from setlexsem.constants import PATH_DATA_ROOT\n",
    "\n",
    "llm_name = \"Claude Haiku\"\n",
    "\n",
    "# Define the directory where your CSV files are located\n",
    "data_path = os.path.join(PATH_DATA_ROOT, \"decile_words\")\n",
    "\n",
    "# Create a dictionary to store DataFrames for each string pattern match\n",
    "dfs_by_deciles = {}\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for file in os.listdir(data_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(os.path.join(data_path, file))\n",
    "        # Iterate through patterns \"Item-{i}\" for i in range(1,10)\n",
    "        for i in range(1, 10):\n",
    "            pattern = f\"Decile-{i}\"\n",
    "            # If pattern is found in the file name\n",
    "            if pattern in file:\n",
    "                # If i-specific list doesn't exist, create one\n",
    "                for j in range(1, 6):\n",
    "                    pattern_L = f\"L-{j}\"\n",
    "                    if pattern_L in file:\n",
    "                        key = f\"Decile-{i}_L-{j}\"\n",
    "                        if key not in dfs_by_deciles:\n",
    "                            dfs_by_deciles[key] = []\n",
    "                        # Append DataFrame to corresponding list\n",
    "                        dfs_by_deciles[key].append(df)\n",
    "                        break  # Stop searching for other patterns in the same file\n",
    "\n",
    "unique_words_dict = {key: 0 for key in dfs_by_deciles.keys()}\n",
    "\n",
    "# Unique words for each token length in a decile\n",
    "for i, dfs in dfs_by_deciles.items():\n",
    "\n",
    "    total_word_count = 0\n",
    "    all_words = []\n",
    "\n",
    "    for df in dfs_by_deciles[i]:\n",
    "        # Concatenate sets from both columns into a single list\n",
    "        combined_sets = df.apply(concat_sets, axis=1).tolist()\n",
    "\n",
    "        # Flatten the list of sets into a single list of words\n",
    "        all_words_j = [word for set_ in combined_sets for word in set_]\n",
    "\n",
    "        all_words.extend(all_words_j)\n",
    "\n",
    "    # Count the unique words\n",
    "    unique_word_count = len(set(all_words))\n",
    "\n",
    "    total_word_count += unique_word_count\n",
    "\n",
    "    unique_words_dict[i] = total_word_count\n",
    "\n",
    "\n",
    "def get_decile_and_length(s):\n",
    "    # Use regular expression to find numbers following 'Key-' and 'Value-'\n",
    "    decile = re.search(r\"Decile-(\\d+)\", s).group(1)\n",
    "    L = re.search(r\"L-(\\d+)\", s).group(1)\n",
    "\n",
    "    return decile, L\n",
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "for key, value in unique_words_dict.items():\n",
    "    decile, L = get_decile_and_length(key)\n",
    "    data.append(\n",
    "        {\n",
    "            \"Token frequency\": decile,\n",
    "            \"Token length\": L,\n",
    "            \"Unique Words\": value,\n",
    "            \"LLM\": llm_name,\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_deciles_L = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "matplotlib.rcParams[\"mathtext.rm\"] = \"Bitstream Vera Sans\"\n",
    "matplotlib.rcParams[\"mathtext.it\"] = \"Bitstream Vera Sans:italic\"\n",
    "matplotlib.rcParams[\"mathtext.bf\"] = \"Bitstream Vera Sans:bold\"\n",
    "matplotlib.rcParams[\"mathtext.fontset\"] = \"stix\"\n",
    "matplotlib.rcParams[\"font.family\"] = \"STIXGeneral\"\n",
    "\n",
    "# Set the size of the plot\n",
    "plt.figure(figsize=(12, 6))  # Increase horizontally and vertically\n",
    "\n",
    "df_deciles_L[\"Token frequency\"] = pd.Categorical(\n",
    "    df_deciles_L[\"Token frequency\"],\n",
    "    categories=sorted(df_deciles_L[\"Token frequency\"].unique()),\n",
    ")\n",
    "\n",
    "# Set the font scale\n",
    "sns.set(font_scale=1.5)  # Increase font size by 20%\n",
    "\n",
    "# Create the barplot\n",
    "sns.barplot(\n",
    "    data=df_deciles_L,\n",
    "    x=\"Token frequency\",\n",
    "    y=\"Unique Words\",\n",
    "    hue=\"Token length\",\n",
    ")\n",
    "\n",
    "plt.gca().set_facecolor(\"white\")\n",
    "\n",
    "plt.gca().grid(axis=\"y\", color=\"lightgray\")\n",
    "\n",
    "# Ensure that the x and y axis lines are visible by setting their color and linewidth\n",
    "ax = plt.gca()\n",
    "ax.spines[\"left\"].set_color(\"black\")\n",
    "ax.spines[\"left\"].set_linewidth(1.5)\n",
    "ax.spines[\"bottom\"].set_color(\"black\")\n",
    "ax.spines[\"bottom\"].set_linewidth(1.5)\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(\"Deciles\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"\")\n",
    "\n",
    "# Save the figure as a PDF\n",
    "plt.savefig(\n",
    "    os.path.join(\n",
    "        PATH_ROOT, f\"manuscript/{FOLDER_NAME}/deciles_unique_words.pdf\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
